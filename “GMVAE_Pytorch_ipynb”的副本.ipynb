{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ivywyl/jpmc-task-1/blob/main/%E2%80%9CGMVAE_Pytorch_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7DVAvLhvAFD"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/jariasf/GMVAE/tree/master/pytorch\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />\n",
        "    <br>View Source on GitHub</a></td>\n",
        "  \n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/drive/173A4-xUYCVnc8nKCy1syKRJi7rw8B38V\">\n",
        "        <img src=\"https://www.gstatic.com/devrel-devsite/v741200ba74cbd1989790411f8b27fb588884a771dac0e0472d95190dde1f7e2f/tensorflow/images/lockup.svg\"  height=\"70px\" style=\"padding-bottom:5px;\"  /><br>View TensorFlow Version</a></td>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yemqVAomvDKG"
      },
      "source": [
        "# Gaussian Mixture Variational Autoencoder\n",
        "\n",
        "**Author:** Jhosimar George Arias Figueroa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBY-xrqrvJNo"
      },
      "source": [
        "This notebook contains a pytorch implementation of a Gaussian Mixture Variational Autoencoder (GMVAE) applied to unsupervised clustering. The model is based on the M2 Unsupervised model proposed by Kingma et al. (https://arxiv.org/pdf/1406.5298), where instead of marginalization of the categorical variable, we use the Gumbel-Softmax distribution (https://arxiv.org/pdf/1611.01144) and modify the generative model to represent a Mixture of Gaussians."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA6iJ0iXvRKL"
      },
      "source": [
        "## Load Source Code from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-nl2NIavSQS",
        "outputId": "656882bb-924b-40ca-9003-9904d02d9464",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# clone the github repository to access source code\n",
        "!git clone https://github.com/jariasf/GMVAE.git\n",
        "\n",
        "# set the correct directory\n",
        "%cd GMVAE/pytorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'GMVAE' already exists and is not an empty directory.\n",
            "/content/GMVAE/pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nLyP7nA-WGZ"
      },
      "source": [
        "## Install Latest Version of Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxNZpTRn-UY3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adae48aa-13ff-443e-8481-7c21530eb891",
        "collapsed": true
      },
      "source": [
        "# tested with torch-1.3.0 torchvision-0.4.1\n",
        "!pip install --upgrade torch torchvision"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGzTb0oswH5J"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThVeaB3CwI4j"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.utils.data\n",
        "from scipy.io import loadmat\n",
        "from model.GMVAE import *\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QNKItMHwQU2"
      },
      "source": [
        "## Input Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tBHkTQ4wVIG"
      },
      "source": [
        "#########################################################\n",
        "## Input Parameters\n",
        "#########################################################\n",
        "parser = argparse.ArgumentParser(description='PyTorch Implementation of DGM Clustering')\n",
        "\n",
        "## Used only in notebooks\n",
        "parser.add_argument('-f', '--file',\n",
        "                    help='Path for input file. First line should contain number of lines to search in')\n",
        "\n",
        "## Dataset\n",
        "parser.add_argument('--dataset', type=str,\n",
        "                    default='protein', help='dataset (default: protein)')\n",
        "parser.add_argument('--seed', type=int, default=1, help='random seed (default: 1)')\n",
        "\n",
        "## GPU\n",
        "parser.add_argument('--cuda', type=int, default=1,\n",
        "                    help='use of cuda (default: 1)')\n",
        "parser.add_argument('--gpuID', type=int, default=0,\n",
        "                    help='set gpu id to use (default: 0)')\n",
        "\n",
        "## Training\n",
        "parser.add_argument('--epochs', type=int, default=100,\n",
        "                    help='number of total epochs to run (default: 200)')\n",
        "parser.add_argument('--batch_size', default=64, type=int,\n",
        "                    help='mini-batch size (default: 64)')\n",
        "parser.add_argument('--batch_size_val', default=200, type=int,\n",
        "                    help='mini-batch size of validation (default: 200)')\n",
        "parser.add_argument('--learning_rate', default=1e-3, type=float,\n",
        "                    help='learning rate (default: 0.001)')\n",
        "parser.add_argument('--decay_epoch', default=-1, type=int,\n",
        "                    help='Reduces the learning rate every decay_epoch')\n",
        "parser.add_argument('--lr_decay', default=0.5, type=float,\n",
        "                    help='Learning rate decay for training (default: 0.5)')\n",
        "\n",
        "## Architecture\n",
        "parser.add_argument('--num_classes', type=int, default=5,\n",
        "                    help='number of classes (default: 5)')\n",
        "parser.add_argument('--gaussian_size', default=64, type=int,\n",
        "                    help='gaussian size (default: 64)')\n",
        "parser.add_argument('--input_size', default=5347, type=int,\n",
        "                    help='input size (default: 5347)')\n",
        "\n",
        "## Partition parameters\n",
        "parser.add_argument('--train_proportion', default=1.0, type=float,\n",
        "                    help='proportion of examples to consider for training only (default: 1.0)')\n",
        "\n",
        "## Gumbel parameters\n",
        "parser.add_argument('--init_temp', default=1.3, type=float,\n",
        "                    help='Initial temperature used in gumbel-softmax (recommended 0.5-1.3, default:1.3)')\n",
        "parser.add_argument('--decay_temp', default=1, type=int,\n",
        "                    help='Set 1 to decay gumbel temperature at every epoch (default: 1)')\n",
        "parser.add_argument('--hard_gumbel', default=0, type=int,\n",
        "                    help='Set 1 to use the hard version of gumbel-softmax (default: 1)')\n",
        "parser.add_argument('--min_temp', default=0.5, type=float,\n",
        "                    help='Minimum temperature of gumbel-softmax after annealing (default: 0.5)' )\n",
        "parser.add_argument('--decay_temp_rate', default=0.013862944, type=float,\n",
        "                    help='Temperature decay rate at every epoch (default: 0.013862944)')\n",
        "\n",
        "## Loss function parameters\n",
        "parser.add_argument('--w_gauss', default=2, type=float,\n",
        "                    help='weight of gaussian loss (default: 1)')\n",
        "parser.add_argument('--w_categ', default=1, type=float,\n",
        "                    help='weight of categorical loss (default: 1)')\n",
        "parser.add_argument('--w_rec', default=1, type=float,\n",
        "                    help='weight of reconstruction loss (default: 1)')\n",
        "parser.add_argument('--rec_type', type=str, choices=['bce', 'mse'],\n",
        "                    default='mse', help='desired reconstruction loss function (default: mse)')\n",
        "\n",
        "## Others\n",
        "parser.add_argument('--verbose', default=0, type=int,\n",
        "                    help='print extra information at every epoch.(default: 0)')\n",
        "\n",
        "args = parser.parse_args()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bti6lPHawc9z"
      },
      "source": [
        "Set random seed in case it was specified in the parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpWPxTy1wgbJ"
      },
      "source": [
        "## Random Seed\n",
        "SEED = args.seed\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if args.cuda:\n",
        "  torch.cuda.manual_seed(SEED)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx729Gd8whg3"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4ewFwVyYdoH"
      },
      "source": [
        "The MNIST dataset consists of 70000 handwritten digits of 28×28 pixel size and 10 classes, of which 60000 images are considered for training and 10000 images for testing. This dataset can be obtained directly from the [torchvision](https://pytorch.org/docs/stable/torchvision/datasets.html) framework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tWeDE6JNzxw",
        "outputId": "bf0b5d39-9c46-49dc-ffc6-0c07c95c6fd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        }
      },
      "source": [
        "print(\"Loading protein dataset...\")\n",
        "\n",
        "# load data\n",
        "class HumanSampleDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = torch.FloatTensor(data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], 0  # 返回数据和dummy label\n",
        "\n",
        "# 加载你的蛋白质数据\n",
        "protein_df = pd.read_csv('/content/GMVAE/pytorch/protein.csv', index_col=0)  # 第一列是基因ID\n",
        "print(f\"原始数据形状: {protein_df.shape}\")\n",
        "print(f\"基因数量: {protein_df.shape[0]}, 人类样本数量: {protein_df.shape[1]}\")\n",
        "\n",
        "# 转置数据：现在每行是一个人类样本，每列是一个基因的表达值\n",
        "human_samples_data = protein_df.T  # shape: (n_humans, n_genes)\n",
        "sample_ids = human_samples_data.index.tolist()  # 人类样本ID\n",
        "gene_ids = human_samples_data.columns.tolist()  # 基因ID\n",
        "\n",
        "print(f\"聚类目标：{len(sample_ids)}个人类样本\")\n",
        "print(f\"特征数量：{len(gene_ids)}个基因表达值\")\n",
        "\n",
        "# 数据标准化\n",
        "scaler = StandardScaler()\n",
        "data_normalized = scaler.fit_transform(human_samples_data.values)\n",
        "\n",
        "# 创建数据集\n",
        "train_dataset = HumanSampleDataset(data_normalized)\n",
        "test_dataset = HumanSampleDataset(data_normalized)\n",
        "\n",
        "# 创建数据加载器\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# print dataset shape\n",
        "print('Train size: ', len(train_dataset), ' Test size: ', len(test_dataset))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading protein dataset...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 5775 fields in line 31, saw 7256\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-583720840.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 加载你的蛋白质数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprotein_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/GMVAE/pytorch/protein.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 第一列是基因ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"原始数据形状: {protein_df.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"基因数量: {protein_df.shape[0]}, 人类样本数量: {protein_df.shape[1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 5775 fields in line 31, saw 7256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmrkmhttwkiD"
      },
      "source": [
        "## Data Partition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUuTFMBhb2vZ"
      },
      "source": [
        "We split the training data into train and validation according to the *train_proportion* parameter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8F6pjC7cNX7"
      },
      "source": [
        "args.batch_size = 64\n",
        "args.batch_size_val = 128\n",
        "\n",
        "def partition_dataset(n, proportion=0.8):\n",
        "  train_num = int(n * proportion)\n",
        "  indices = np.random.permutation(n)\n",
        "  train_indices, val_indices = indices[:train_num], indices[train_num:]\n",
        "  return train_indices, val_indices\n",
        "\n",
        "if args.train_proportion == 1.0:\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size_val, shuffle=False)\n",
        "  val_loader = test_loader\n",
        "else:\n",
        "  train_indices, val_indices = partition_dataset(len(train_dataset), args.train_proportion)\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, sampler=SubsetRandomSampler(train_indices))\n",
        "  val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size_val, sampler=SubsetRandomSampler(val_indices))\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size_val, shuffle=False)\n",
        "\n",
        "args.input_size = 5347  # 基因数量作为输入维度\n",
        "args.num_classes = 5  # 人类聚类数量，可以根据需要调整\n",
        "args.gaussian_size = 64  # 保持原默认值即可\n",
        "\n",
        "print(f\"输入维度: {args.input_size} (基因数量)\")\n",
        "print(f\"聚类数量: {args.num_classes}\")\n",
        "print(f\"样本数量: {len(sample_ids)} (人类样本)\")\n",
        "print(f\"高斯维度: {args.gaussian_size}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt7sEfZWw_U7"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTncRSCuxFEL"
      },
      "source": [
        "# Model Initialization\n",
        "gmvae = GMVAE(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQx-EzIexJFV"
      },
      "source": [
        "# Training Phase\n",
        "history_loss = gmvae.train(train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CimQtyl1foCj"
      },
      "source": [
        "## Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs2-BLGkfp8m"
      },
      "source": [
        "print(\"Getting clustering results...\")\n",
        "# 1. 设置网络为评估模式\n",
        "gmvae.network.eval()\n",
        "\n",
        "# 2. 使用GMVAE自带的test方法获取基本结果\n",
        "accuracy, nmi = gmvae.test(test_loader)\n",
        "print(f\"Accuracy: {accuracy:.5f}, NMI: {nmi:.5f}\")\n",
        "\n",
        "# 3. 如果需要详细的聚类分配，手动获取\n",
        "gmvae.network.eval()\n",
        "all_cluster_labels = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, _ in test_loader:\n",
        "        if gmvae.cuda:\n",
        "            data = data.cuda()\n",
        "\n",
        "        # 使用network进行前向传播\n",
        "        out_net = gmvae.network(data, gmvae.gumbel_temp, gmvae.hard_gumbel)\n",
        "\n",
        "        # 检查输出结构\n",
        "        print(\"网络输出的键:\", list(out_net.keys()))  # 先看看有什么输出\n",
        "\n",
        "        # 根据输出键获取聚类概率\n",
        "        if 'y_prob' in out_net:\n",
        "            pi = out_net['y_prob']\n",
        "        elif 'categorical' in out_net:\n",
        "            pi = out_net['categorical']\n",
        "        elif 'y' in out_net:\n",
        "            pi = out_net['y']\n",
        "        else:\n",
        "            # 如果不确定，打印所有输出看看\n",
        "            for key, value in out_net.items():\n",
        "                print(f\"{key}: shape {value.shape}\")\n",
        "            pi = list(out_net.values())[1]  # 通常第二个是聚类概率\n",
        "\n",
        "        cluster_labels = torch.argmax(pi, dim=1)\n",
        "        all_cluster_labels.append(cluster_labels.cpu())\n",
        "        all_probabilities.append(pi.cpu())\n",
        "\n",
        "        break  # 先只处理第一个batch看看输出结构\n",
        "\n",
        "# 查看第一个batch的结果\n",
        "print(f\"第一个batch的聚类标签: {all_cluster_labels[0]}\")\n",
        "print(f\"聚类概率形状: {all_probabilities[0].shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bHWNxV-gAl2"
      },
      "source": [
        "## Complete Result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 更正后的聚类结果获取\n",
        "def get_clustering_results_corrected(gmvae, test_loader, sample_ids):\n",
        "    print(\"获取聚类结果...\")\n",
        "    gmvae.network.eval()\n",
        "\n",
        "    all_cluster_labels = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            if gmvae.cuda:\n",
        "                data = data.cuda()\n",
        "\n",
        "            out_net = gmvae.network(data, gmvae.gumbel_temp, gmvae.hard_gumbel)\n",
        "\n",
        "            # 优先级顺序选择聚类概率\n",
        "            if 'prob_cat' in out_net:\n",
        "                pi = out_net['prob_cat']  # 最可能是聚类概率\n",
        "                print(\"使用 prob_cat 作为聚类概率\")\n",
        "            elif 'categorical' in out_net:\n",
        "                pi = out_net['categorical']  # 次优选择\n",
        "                print(\"使用 categorical 作为聚类概率\")\n",
        "            else:\n",
        "                pi = torch.softmax(out_net['logits'], dim=1)  # 从logits计算\n",
        "                print(\"从 logits 计算聚类概率\")\n",
        "\n",
        "            cluster_labels = torch.argmax(pi, dim=1)\n",
        "            all_cluster_labels.append(cluster_labels.cpu())\n",
        "            all_probabilities.append(pi.cpu())\n",
        "\n",
        "    # 合并结果\n",
        "    all_cluster_labels = torch.cat(all_cluster_labels, dim=0).numpy()\n",
        "    all_probabilities = torch.cat(all_probabilities, dim=0).numpy()\n",
        "\n",
        "    return all_cluster_labels, all_probabilities\n",
        "\n",
        "# 运行修正版本\n",
        "cluster_labels, probabilities = get_clustering_results_corrected(gmvae, test_loader, sample_ids)\n",
        "\n",
        "# 创建详细结果\n",
        "cluster_results = pd.DataFrame({\n",
        "    'Sample_ID': sample_ids,\n",
        "    'Cluster': cluster_labels,\n",
        "    'Max_Probability': np.max(probabilities, axis=1)\n",
        "})\n",
        "\n",
        "print(\"=== 修正后的聚类结果 ===\")\n",
        "for cluster_id in sorted(cluster_results['Cluster'].unique()):\n",
        "    cluster_samples = cluster_results[cluster_results['Cluster'] == cluster_id]\n",
        "    print(f\"聚类 {cluster_id}: {len(cluster_samples)} 个样本 ({len(cluster_samples)/len(cluster_results)*100:.1f}%)\")\n",
        "    print(f\"  平均置信度: {cluster_samples['Max_Probability'].mean():.3f}\")"
      ],
      "metadata": {
        "id": "_qLl0OzR2BLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# 1. 创建结果保存目录\n",
        "results_dir = 'clustering_results'\n",
        "if not os.path.exists(results_dir):\n",
        "    os.makedirs(results_dir)\n",
        "    print(f\"创建结果目录: {results_dir}\")\n",
        "\n",
        "# 2. 生成时间戳用于文件命名\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# 3. 保存主要结果文件\n",
        "main_results_file = f'{results_dir}/gmvae_clustering_results_{timestamp}.csv'\n",
        "cluster_results.to_csv(main_results_file, index=False)\n",
        "print(f\"主要聚类结果已保存到: {main_results_file}\")\n",
        "\n",
        "# 4. 保存概率矩阵（所有聚类的概率）\n",
        "prob_matrix_file = f'{results_dir}/clustering_probabilities_{timestamp}.csv'\n",
        "prob_df = pd.DataFrame(probabilities,\n",
        "                      columns=[f'Cluster_{i}_Prob' for i in range(probabilities.shape[1])])\n",
        "prob_df['Sample_ID'] = sample_ids\n",
        "prob_df = prob_df[['Sample_ID'] + [col for col in prob_df.columns if col != 'Sample_ID']]\n",
        "prob_df.to_csv(prob_matrix_file, index=False)\n",
        "print(f\"概率矩阵已保存到: {prob_matrix_file}\")\n",
        "\n",
        "# 5. 保存聚类统计摘要\n",
        "summary_file = f'{results_dir}/clustering_summary_{timestamp}.txt'\n",
        "with open(summary_file, 'w') as f:\n",
        "    f.write(\"GMVAE 聚类结果摘要\\n\")\n",
        "    f.write(\"=\" * 50 + \"\\n\")\n",
        "    f.write(f\"总样本数: {len(cluster_results)}\\n\")\n",
        "    f.write(f\"设置聚类数: {probabilities.shape[1]}\\n\")\n",
        "    f.write(f\"实际使用聚类数: {len(cluster_results['Cluster'].unique())}\\n\")\n",
        "    f.write(f\"平均置信度: {cluster_results['Max_Probability'].mean():.4f}\\n\")\n",
        "    f.write(f\"置信度标准差: {cluster_results['Max_Probability'].std():.4f}\\n\\n\")\n",
        "\n",
        "    f.write(\"各聚类详细信息:\\n\")\n",
        "    f.write(\"-\" * 30 + \"\\n\")\n",
        "    for cluster_id in sorted(cluster_results['Cluster'].unique()):\n",
        "        cluster_samples = cluster_results[cluster_results['Cluster'] == cluster_id]\n",
        "        f.write(f\"聚类 {cluster_id}:\\n\")\n",
        "        f.write(f\"  样本数量: {len(cluster_samples)} ({len(cluster_samples)/len(cluster_results)*100:.1f}%)\\n\")\n",
        "        f.write(f\"  平均置信度: {cluster_samples['Max_Probability'].mean():.4f}\\n\")\n",
        "        f.write(f\"  置信度范围: {cluster_samples['Max_Probability'].min():.4f} - {cluster_samples['Max_Probability'].max():.4f}\\n\")\n",
        "\n",
        "        # 显示前5个高置信度样本\n",
        "        top_samples = cluster_samples.nlargest(5, 'Max_Probability')\n",
        "        f.write(f\"  高置信度样本示例: {', '.join(top_samples['Sample_ID'].tolist())}\\n\\n\")\n",
        "\n",
        "print(f\"聚类摘要已保存到: {summary_file}\")\n",
        "\n",
        "# 6. 保存每个聚类的样本列表\n",
        "for cluster_id in sorted(cluster_results['Cluster'].unique()):\n",
        "    cluster_samples = cluster_results[cluster_results['Cluster'] == cluster_id]\n",
        "    cluster_file = f'{results_dir}/cluster_{cluster_id}_samples_{timestamp}.txt'\n",
        "\n",
        "    with open(cluster_file, 'w') as f:\n",
        "        f.write(f\"聚类 {cluster_id} 样本列表\\n\")\n",
        "        f.write(f\"样本数量: {len(cluster_samples)}\\n\")\n",
        "        f.write(f\"平均置信度: {cluster_samples['Max_Probability'].mean():.4f}\\n\")\n",
        "        f.write(\"=\" * 40 + \"\\n\")\n",
        "\n",
        "        # 按置信度排序\n",
        "        sorted_samples = cluster_samples.sort_values('Max_Probability', ascending=False)\n",
        "        for _, sample in sorted_samples.iterrows():\n",
        "            f.write(f\"{sample['Sample_ID']}\\t{sample['Max_Probability']:.4f}\\n\")\n",
        "\n",
        "    print(f\"聚类 {cluster_id} 样本列表已保存到: {cluster_file}\")\n",
        "\n",
        "# 7. 输出文件清单\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(f\"所有结果文件已保存到目录: {results_dir}\")\n",
        "print(f\"文件清单:\")\n",
        "print(f\"  主要结果: {os.path.basename(main_results_file)}\")\n",
        "print(f\"  概率矩阵: {os.path.basename(prob_matrix_file)}\")\n",
        "print(f\"  统计摘要: {os.path.basename(summary_file)}\")\n",
        "for cluster_id in sorted(cluster_results['Cluster'].unique()):\n",
        "    print(f\"  聚类{cluster_id}样本: cluster_{cluster_id}_samples_{timestamp}.txt\")\n",
        "print(f\"=\"*60)\n"
      ],
      "metadata": {
        "id": "m0ivg7FLAJ8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-C7hRxZAnkYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SbXR7FkhIcq"
      },
      "source": [
        "## Visualization of the feature latent space"
      ]
    }
  ]
}